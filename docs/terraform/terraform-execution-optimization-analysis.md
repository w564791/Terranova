# Terraformæ‰§è¡Œæµç¨‹ä¼˜åŒ–åˆ†æ

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025-11-08  
> **çŠ¶æ€**: ä¼˜åŒ–å»ºè®®åˆ†æ  
> **ç›¸å…³æ–‡æ¡£**: [terraform-execution-states-and-sequential-guarantee.md](terraform-execution-states-and-sequential-guarantee.md)

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£åˆ†æTerraformæ‰§è¡Œæµç¨‹ä¸­çš„ä¸¤ä¸ªå…³é”®ä¼˜åŒ–ç‚¹ï¼Œè¯„ä¼°å…¶åˆç†æ€§ã€å¯è¡Œæ€§å’Œå®æ–½æ–¹æ¡ˆã€‚

## ğŸ¯ ä¼˜åŒ–ç‚¹1: Planåˆ°Applyçš„å·¥ä½œç›®å½•ä¿æŒ

### å½“å‰å®ç°é—®é¢˜

```go
// å½“å‰æµç¨‹
Plané˜¶æ®µ:
1. åˆ›å»ºå·¥ä½œç›®å½• /tmp/iac-platform/workspaces/{ws_id}/{task_id}/
2. ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼ˆmain.tf.json, provider.tf.jsonç­‰ï¼‰
3. æ‹‰å–Stateæ–‡ä»¶
4. terraform init
5. terraform plan -out=plan.out
6. ä¿å­˜plan.outåˆ°æ•°æ®åº“
7. æ¸…ç†å·¥ä½œç›®å½• âŒ

Applyé˜¶æ®µï¼ˆç”¨æˆ·ç¡®è®¤åï¼‰:
1. é‡æ–°åˆ›å»ºå·¥ä½œç›®å½•
2. é‡æ–°ç”Ÿæˆé…ç½®æ–‡ä»¶ âŒ é‡å¤
3. é‡æ–°æ‹‰å–Stateæ–‡ä»¶ âŒ é‡å¤
4. terraform init âŒ é‡å¤
5. ä»æ•°æ®åº“æ¢å¤plan.out
6. terraform apply plan.out
7. æ¸…ç†å·¥ä½œç›®å½•
```

**é—®é¢˜**:
- é‡å¤æ‰§è¡Œinitï¼ˆä¸‹è½½Provideræ’ä»¶ï¼Œè€—æ—¶ï¼‰
- é‡å¤ç”Ÿæˆé…ç½®æ–‡ä»¶
- é‡å¤æ‹‰å–Stateæ–‡ä»¶
- å¢åŠ äº†Applyé˜¶æ®µçš„å¯åŠ¨æ—¶é—´

### ä¼˜åŒ–æ–¹æ¡ˆ

#### æ–¹æ¡ˆA: ä¿æŒå·¥ä½œç›®å½•ï¼ˆæ¨èï¼‰

```go
Plané˜¶æ®µ:
1. åˆ›å»ºå·¥ä½œç›®å½• /tmp/iac-platform/workspaces/{ws_id}/{task_id}/
2. ç”Ÿæˆé…ç½®æ–‡ä»¶
3. æ‹‰å–Stateæ–‡ä»¶
4. terraform init
5. terraform plan -out=plan.out
6. ä¿å­˜plan.outåˆ°æ•°æ®åº“
7. è®¡ç®—å·¥ä½œç›®å½•hashï¼ˆå¯é€‰ï¼Œç”¨äºéªŒè¯ï¼‰
8. ä¿æŒå·¥ä½œç›®å½•ä¸æ¸…ç† 

Applyé˜¶æ®µ:
1. éªŒè¯å·¥ä½œç›®å½•æ˜¯å¦å­˜åœ¨
2. éªŒè¯plan.outæ–‡ä»¶hashï¼ˆå¯é€‰ï¼‰
3. ç›´æ¥æ‰§è¡Œ terraform apply plan.out 
4. æ¸…ç†å·¥ä½œç›®å½•
```

**ä¼˜ç‚¹**:
-  èŠ‚çœinitæ—¶é—´ï¼ˆé€šå¸¸5-30ç§’ï¼‰
-  èŠ‚çœé…ç½®æ–‡ä»¶ç”Ÿæˆæ—¶é—´
-  èŠ‚çœStateæ–‡ä»¶æ‹‰å–æ—¶é—´
-  å‡å°‘ç½‘ç»œIO
-  å‡å°‘ç£ç›˜IO
-  Applyå¯åŠ¨æ›´å¿«

**ç¼ºç‚¹**:
- âŒ å ç”¨ç£ç›˜ç©ºé—´ï¼ˆç›´åˆ°Applyå®Œæˆï¼‰
- âŒ éœ€è¦å¤„ç†å·¥ä½œç›®å½•ä¸¢å¤±çš„æƒ…å†µ
- âŒ éœ€è¦å¤„ç†æœåŠ¡å™¨é‡å¯çš„æƒ…å†µ

#### æ–¹æ¡ˆB: ä½¿ç”¨Planæ–‡ä»¶hashéªŒè¯ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰

```go
Plané˜¶æ®µ:
1. æ‰§è¡ŒPlan
2. ä¿å­˜plan.outåˆ°æ•°æ®åº“
3. è®¡ç®—plan.outçš„hash
4. ä¿å­˜hashåˆ°task.plan_hashå­—æ®µ
5. æ¸…ç†å·¥ä½œç›®å½•

Applyé˜¶æ®µ:
1. åˆ›å»ºå·¥ä½œç›®å½•
2. ä»æ•°æ®åº“æ¢å¤plan.out
3. éªŒè¯plan.outçš„hash 
4. å¦‚æœhashåŒ¹é…ï¼Œè·³è¿‡initï¼Œç›´æ¥apply 
5. å¦‚æœhashä¸åŒ¹é…ï¼Œé‡æ–°initåapply
```

**ä¼˜ç‚¹**:
-  æä¾›äº†æ•°æ®å®Œæ•´æ€§éªŒè¯
-  ä¸å ç”¨é•¿æœŸç£ç›˜ç©ºé—´
-  å¯ä»¥æ£€æµ‹planæ–‡ä»¶æŸå

**ç¼ºç‚¹**:
- âŒ ä»ç„¶éœ€è¦é‡æ–°ç”Ÿæˆé…ç½®æ–‡ä»¶
- âŒ ä»ç„¶éœ€è¦é‡æ–°æ‹‰å–State
- âŒ ä»ç„¶éœ€è¦initï¼ˆè™½ç„¶å¯ä»¥ä¼˜åŒ–ï¼‰

### åˆç†æ€§è¯„ä¼°

| ç»´åº¦ | æ–¹æ¡ˆAï¼ˆä¿æŒç›®å½•ï¼‰ | æ–¹æ¡ˆBï¼ˆhashéªŒè¯ï¼‰ | è¯„åˆ† |
|------|------------------|------------------|------|
| æ€§èƒ½æå‡ | â­â­â­â­â­ | â­â­â­ | Aæ›´ä¼˜ |
| å®ç°å¤æ‚åº¦ | â­â­â­ | â­â­â­â­ | ç›¸å½“ |
| å¯é æ€§ | â­â­â­â­ | â­â­â­â­â­ | Bæ›´ä¼˜ |
| èµ„æºå ç”¨ | â­â­â­ | â­â­â­â­â­ | Bæ›´ä¼˜ |

#### æ–¹æ¡ˆC: æ··åˆæ–¹æ¡ˆ - ä¿æŒç›®å½• + HashéªŒè¯ + Agentæ„ŸçŸ¥ï¼ˆæœ€ä¼˜æ–¹æ¡ˆï¼‰

```go
Plané˜¶æ®µ:
1. åˆ›å»ºå·¥ä½œç›®å½• /tmp/iac-platform/workspaces/{ws_id}/{task_id}/
2. ç”Ÿæˆé…ç½®æ–‡ä»¶
3. æ‹‰å–Stateæ–‡ä»¶
4. terraform init
5. terraform plan -out=plan.out
6. è®¡ç®—plan.outçš„hashå¹¶ä¿å­˜åˆ°task.plan_hash 
7. ä¿å­˜plan.outåˆ°æ•°æ®åº“ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
8. ä¿æŒå·¥ä½œç›®å½•ä¸æ¸…ç† 
9. è®°å½•æ‰§è¡ŒAgent IDåˆ°task.warmup_agent_id 

Applyé˜¶æ®µï¼ˆAgentæœªé”€æ¯ï¼‰:
1. æ£€æŸ¥å½“å‰Agent IDæ˜¯å¦ç­‰äºwarmup_agent_id 
2. å¦‚æœç›¸åŒï¼ŒéªŒè¯æœ¬åœ°plan.outçš„hash 
3. HashåŒ¹é… â†’ ç›´æ¥æ‰§è¡Œapplyï¼ˆæœ€å¿«è·¯å¾„ï¼‰
4. Hashä¸åŒ¹é… â†’ ä»æ•°æ®åº“æ¢å¤plan.out
5. æ‰§è¡Œterraform apply

Applyé˜¶æ®µï¼ˆAgentå·²é”€æ¯/é‡å»ºï¼‰:
1. æ–°Agentå¯åŠ¨æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰apply_pendingä»»åŠ¡ 
2. å¦‚æœæœ‰ï¼Œè‡ªåŠ¨æ‰§è¡Œé¢„çƒ­æµç¨‹: 
   - åˆ›å»ºå·¥ä½œç›®å½•
   - ç”Ÿæˆé…ç½®æ–‡ä»¶
   - æ‹‰å–Stateæ–‡ä»¶
   - terraform init
   - ä»æ•°æ®åº“æ¢å¤plan.out
   - éªŒè¯hash
   - æ ‡è®°ä¸ºready
3. ç­‰å¾…ç”¨æˆ·confirm apply
4. ç”¨æˆ·ç¡®è®¤åç«‹å³æ‰§è¡Œapply
```

**ä¼˜ç‚¹**:
-  Agentæœªé”€æ¯æ—¶æ€§èƒ½æœ€ä¼˜ï¼ˆç›´æ¥applyï¼Œæ— éœ€initï¼‰
-  Agenté”€æ¯åè‡ªåŠ¨é¢„çƒ­ï¼Œç”¨æˆ·æ— æ„ŸçŸ¥
-  HashéªŒè¯ä¿è¯æ•°æ®å®Œæ•´æ€§
-  æ•°æ®åº“å¤‡ä»½æä¾›å®¹ç¾èƒ½åŠ›
-  ç”¨æˆ·ä½“éªŒæœ€ä½³

**ç¼ºç‚¹**:
-  å®ç°å¤æ‚åº¦è¾ƒé«˜
-  éœ€è¦Agentæ„ŸçŸ¥æœºåˆ¶

**æ¨è**: **æ–¹æ¡ˆCï¼ˆæ··åˆæ–¹æ¡ˆï¼‰** â­â­â­â­â­

**ç†ç”±**:
1. ç»“åˆäº†æ–¹æ¡ˆAå’Œæ–¹æ¡ˆBçš„æ‰€æœ‰ä¼˜ç‚¹
2. å¤„ç†äº†Agenté”€æ¯é‡å»ºçš„åœºæ™¯
3. æ€§èƒ½å’Œå¯é æ€§éƒ½è¾¾åˆ°æœ€ä¼˜
4. ç”¨æˆ·ä½“éªŒæœ€å¥½

## ğŸ¯ ä¼˜åŒ–ç‚¹2: Agenté¢„çƒ­æœºåˆ¶

### å½“å‰å®ç°é—®é¢˜

```go
// å½“å‰æµç¨‹
Planå®Œæˆ â†’ apply_pendingçŠ¶æ€ â†’ ç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼ˆå¯èƒ½å¾ˆä¹…ï¼‰
                                    â†“
ç”¨æˆ·ç¡®è®¤ â†’ Agentæ¥æ”¶ä»»åŠ¡ â†’ åˆ›å»ºå·¥ä½œç›®å½• â†’ ç”Ÿæˆé…ç½® â†’ æ‹‰å–State â†’ init â†’ apply
                          â†‘
                          è¿™äº›æ­¥éª¤è€—æ—¶è¾ƒé•¿ï¼ˆ10-60ç§’ï¼‰
```

**é—®é¢˜**:
- ç”¨æˆ·ç¡®è®¤åéœ€è¦ç­‰å¾…è¾ƒé•¿æ—¶é—´æ‰èƒ½çœ‹åˆ°Applyå¼€å§‹
- ç”¨æˆ·ä½“éªŒä¸å¥½
- åœ¨Agenté‡å¯/é”€æ¯é‡å»ºçš„åœºæ™¯ä¸‹ï¼Œé—®é¢˜æ›´æ˜æ˜¾

### ä¼˜åŒ–æ–¹æ¡ˆ

#### æ–¹æ¡ˆA: Agentä»»åŠ¡é¢„çƒ­ï¼ˆæ¨èï¼‰

```go
// ä¼˜åŒ–åæµç¨‹
Planå®Œæˆ â†’ apply_pendingçŠ¶æ€ â†’ ç«‹å³æ¨é€"é¢„çƒ­ä»»åŠ¡"åˆ°Agent
                                    â†“
                              Agenté¢„çƒ­:
                              1. åˆ›å»ºå·¥ä½œç›®å½•
                              2. ç”Ÿæˆé…ç½®æ–‡ä»¶
                              3. æ‹‰å–Stateæ–‡ä»¶
                              4. terraform init
                              5. ä»æ•°æ®åº“æ¢å¤plan.out
                              6. æ ‡è®°ä¸º"ready"çŠ¶æ€
                              7. ç­‰å¾…ç”¨æˆ·ç¡®è®¤
                                    â†“
ç”¨æˆ·ç¡®è®¤ â†’ Agentç«‹å³æ‰§è¡Œ terraform apply plan.out 
          ï¼ˆå‡ ä¹æ— å»¶è¿Ÿï¼‰
```

**å®ç°ç»†èŠ‚**:

```go
// 1. æ–°å¢ä»»åŠ¡é¢„çƒ­çŠ¶æ€
type TaskWarmupStatus string

const (
    WarmupStatusNone       TaskWarmupStatus = "none"        // æœªé¢„çƒ­
    WarmupStatusWarming    TaskWarmupStatus = "warming"     // é¢„çƒ­ä¸­
    WarmupStatusReady      TaskWarmupStatus = "ready"       // é¢„çƒ­å®Œæˆ
    WarmupStatusExpired    TaskWarmupStatus = "expired"     // é¢„çƒ­è¿‡æœŸ
)

// 2. åœ¨WorkspaceTaskä¸­æ·»åŠ å­—æ®µ
type WorkspaceTask struct {
    // ... ç°æœ‰å­—æ®µ
    WarmupStatus    TaskWarmupStatus `json:"warmup_status" gorm:"default:none"`
    WarmupAgentID   *string          `json:"warmup_agent_id"`
    WarmupAt        *time.Time       `json:"warmup_at"`
    WarmupExpiresAt *time.Time       `json:"warmup_expires_at"` // é¢„çƒ­è¿‡æœŸæ—¶é—´
}

// 3. Planå®Œæˆåè§¦å‘é¢„çƒ­
func (e *TerraformExecutor) OnPlanComplete(task *models.WorkspaceTask) error {
    if task.TaskType == models.TaskTypePlanAndApply {
        // æ¨é€é¢„çƒ­ä»»åŠ¡åˆ°Agent
        return e.warmupTaskOnAgent(task)
    }
    return nil
}

// 4. Agenté¢„çƒ­é€»è¾‘
func (a *Agent) WarmupTask(taskID uint) error {
    // 1. åˆ›å»ºå·¥ä½œç›®å½•
    workDir := a.createWorkDir(taskID)
    
    // 2. ç”Ÿæˆé…ç½®æ–‡ä»¶
    if err := a.generateConfigFiles(taskID, workDir); err != nil {
        return err
    }
    
    // 3. æ‹‰å–State
    if err := a.fetchState(taskID, workDir); err != nil {
        return err
    }
    
    // 4. Terraform init
    if err := a.terraformInit(workDir); err != nil {
        return err
    }
    
    // 5. æ¢å¤plan.out
    if err := a.restorePlanFile(taskID, workDir); err != nil {
        return err
    }
    
    // 6. æ ‡è®°ä¸ºready
    return a.updateTaskWarmupStatus(taskID, WarmupStatusReady)
}

// 5. ç”¨æˆ·ç¡®è®¤åç›´æ¥æ‰§è¡Œ
func (a *Agent) ExecuteApply(taskID uint) error {
    task := a.getTask(taskID)
    
    if task.WarmupStatus == WarmupStatusReady {
        // å·¥ä½œç›®å½•å·²å‡†å¤‡å¥½ï¼Œç›´æ¥æ‰§è¡Œ
        return a.terraformApply(task.WorkDir)
    } else {
        // é¢„çƒ­å¤±è´¥æˆ–è¿‡æœŸï¼Œèµ°æ­£å¸¸æµç¨‹
        return a.executeApplyNormal(taskID)
    }
}
```

**ä¼˜ç‚¹**:
-  ç”¨æˆ·ç¡®è®¤åå‡ ä¹ç«‹å³å¼€å§‹Apply
-  ç”¨æˆ·ä½“éªŒå¤§å¹…æå‡
-  å……åˆ†åˆ©ç”¨ç­‰å¾…æ—¶é—´
-  å¯¹Agenté‡å¯åœºæ™¯å‹å¥½

**ç¼ºç‚¹**:
- âŒ å¢åŠ äº†ç³»ç»Ÿå¤æ‚åº¦
- âŒ éœ€è¦å¤„ç†é¢„çƒ­è¿‡æœŸçš„æƒ…å†µ
- âŒ éœ€è¦å¤„ç†Agentåˆ‡æ¢çš„æƒ…å†µ
- âŒ å ç”¨Agentèµ„æºï¼ˆä½†å¯æ¥å—ï¼‰

#### æ–¹æ¡ˆB: å»¶è¿Ÿé¢„çƒ­ï¼ˆä¿å®ˆæ–¹æ¡ˆï¼‰

```go
// åªåœ¨ç”¨æˆ·å³å°†ç¡®è®¤æ—¶é¢„çƒ­
ç”¨æˆ·æ‰“å¼€"Confirm Apply"å¯¹è¯æ¡† â†’ è§¦å‘é¢„çƒ­
                                    â†“
                              åå°é¢„çƒ­ï¼ˆ5-10ç§’ï¼‰
                                    â†“
ç”¨æˆ·ç‚¹å‡»ç¡®è®¤ â†’ å¦‚æœé¢„çƒ­å®Œæˆï¼Œç«‹å³æ‰§è¡Œ
              å¦‚æœé¢„çƒ­æœªå®Œæˆï¼Œç­‰å¾…é¢„çƒ­å®Œæˆ
```

**ä¼˜ç‚¹**:
-  å‡å°‘ä¸å¿…è¦çš„é¢„çƒ­
-  å®ç°ç›¸å¯¹ç®€å•

**ç¼ºç‚¹**:
- âŒ ä»ç„¶éœ€è¦ç­‰å¾…
- âŒ ç”¨æˆ·ä½“éªŒæå‡æœ‰é™

### åˆç†æ€§è¯„ä¼°

| ç»´åº¦ | æ–¹æ¡ˆAï¼ˆç«‹å³é¢„çƒ­ï¼‰ | æ–¹æ¡ˆBï¼ˆå»¶è¿Ÿé¢„çƒ­ï¼‰ | è¯„åˆ† |
|------|------------------|------------------|------|
| ç”¨æˆ·ä½“éªŒ | â­â­â­â­â­ | â­â­â­ | Aæ›´ä¼˜ |
| å®ç°å¤æ‚åº¦ | â­â­â­ | â­â­â­â­ | Bæ›´ç®€å• |
| èµ„æºåˆ©ç”¨ | â­â­â­â­ | â­â­â­â­â­ | Bæ›´ä¼˜ |
| å¯é æ€§ | â­â­â­â­ | â­â­â­â­â­ | Bæ›´ä¼˜ |

**æ¨è**: **æ–¹æ¡ˆAï¼ˆç«‹å³é¢„çƒ­ï¼‰**

**ç†ç”±**:
1. ç”¨æˆ·ä½“éªŒæå‡æœ€æ˜æ˜¾
2. å……åˆ†åˆ©ç”¨ç­‰å¾…æ—¶é—´
3. å®ç°å¤æ‚åº¦å¯æ¥å—
4. èµ„æºå ç”¨å¯æ§

## ğŸ”„ æ–¹æ¡ˆCè¯¦ç»†å®ç°

### æ•°æ®åº“Schemaå˜æ›´

```sql
-- æ·»åŠ é¢„çƒ­ç›¸å…³å­—æ®µ
ALTER TABLE workspace_tasks ADD COLUMN IF NOT EXISTS plan_hash VARCHAR(64);
ALTER TABLE workspace_tasks ADD COLUMN IF NOT EXISTS warmup_agent_id VARCHAR(50);
ALTER TABLE workspace_tasks ADD COLUMN IF NOT EXISTS warmup_status VARCHAR(20) DEFAULT 'none';
ALTER TABLE workspace_tasks ADD COLUMN IF NOT EXISTS warmup_at TIMESTAMP;
ALTER TABLE workspace_tasks ADD COLUMN IF NOT EXISTS warmup_expires_at TIMESTAMP;

-- æ·»åŠ ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_workspace_tasks_warmup_agent 
ON workspace_tasks(warmup_agent_id, warmup_status);

CREATE INDEX IF NOT EXISTS idx_workspace_tasks_apply_pending_pool
ON workspace_tasks(status, warmup_status) 
WHERE status = 'apply_pending';
```

### æ ¸å¿ƒå®ç°ä»£ç 

```go
// 1. Planå®Œæˆåçš„å¤„ç†
func (e *TerraformExecutor) OnPlanComplete(task *models.WorkspaceTask, workDir string) error {
    // è®¡ç®—plan.outçš„hash
    planFile := filepath.Join(workDir, "plan.out")
    planData, err := os.ReadFile(planFile)
    if err != nil {
        return fmt.Errorf("failed to read plan file: %w", err)
    }
    
    hash := sha256.Sum256(planData)
    task.PlanHash = hex.EncodeToString(hash[:])
    
    // è®°å½•å½“å‰Agent IDï¼ˆå¦‚æœæ˜¯Agentæ¨¡å¼ï¼‰
    if task.ExecutionMode == models.ExecutionModeAgent && task.AgentID != nil {
        task.WarmupAgentID = task.AgentID
    }
    
    // ä¿å­˜åˆ°æ•°æ®åº“
    if err := e.db.Save(task).Error; err != nil {
        return err
    }
    
    // ä¸æ¸…ç†å·¥ä½œç›®å½•ï¼
    log.Printf("[Optimization] Keeping work directory for task %d at %s", task.ID, workDir)
    
    return nil
}

// 2. Agentå¯åŠ¨æ—¶çš„é¢„çƒ­æ£€æŸ¥
func (a *Agent) OnStart() error {
    log.Printf("[Agent] Starting, checking for apply_pending tasks...")
    
    // æŸ¥è¯¢åˆ†é…ç»™å½“å‰poolçš„apply_pendingä»»åŠ¡
    var tasks []models.WorkspaceTask
    err := a.db.Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", a.poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusApplyPending).
        Where("workspace_tasks.warmup_status != ?", "ready").
        Find(&tasks).Error
    
    if err != nil {
        return err
    }
    
    if len(tasks) == 0 {
        log.Printf("[Agent] No apply_pending tasks need warmup")
        return nil
    }
    
    log.Printf("[Agent] Found %d apply_pending tasks, starting warmup...", len(tasks))
    
    // ä¸ºæ¯ä¸ªä»»åŠ¡æ‰§è¡Œé¢„çƒ­
    for _, task := range tasks {
        go a.warmupTask(&task)
    }
    
    return nil
}

// 3. Agenté¢„çƒ­é€»è¾‘
func (a *Agent) warmupTask(task *models.WorkspaceTask) error {
    log.Printf("[Agent] Warming up task %d", task.ID)
    
    // æ›´æ–°é¢„çƒ­çŠ¶æ€
    task.WarmupStatus = "warming"
    task.WarmupAgentID = &a.agentID
    task.WarmupAt = timePtr(time.Now())
    task.WarmupExpiresAt = timePtr(time.Now().Add(1 * time.Hour))
    a.db.Save(task)
    
    // åˆ›å»ºå·¥ä½œç›®å½•
    workDir := fmt.Sprintf("/tmp/iac-platform/workspaces/%s/%d", 
        task.WorkspaceID, task.ID)
    
    if err := os.MkdirAll(workDir, 0755); err != nil {
        return a.handleWarmupError(task, err)
    }
    
    // ç”Ÿæˆé…ç½®æ–‡ä»¶
    if err := a.generateConfigFiles(task, workDir); err != nil {
        return a.handleWarmupError(task, err)
    }
    
    // æ‹‰å–Stateæ–‡ä»¶
    if err := a.fetchStateFile(task, workDir); err != nil {
        return a.handleWarmupError(task, err)
    }
    
    // Terraform init
    if err := a.terraformInit(workDir); err != nil {
        return a.handleWarmupError(task, err)
    }
    
    // ä»æ•°æ®åº“æ¢å¤plan.out
    if err := a.restorePlanFile(task, workDir); err != nil {
        return a.handleWarmupError(task, err)
    }
    
    // éªŒè¯plan.outçš„hash
    planFile := filepath.Join(workDir, "plan.out")
    planData, err := os.ReadFile(planFile)
    if err != nil {
        return a.handleWarmupError(task, err)
    }
    
    hash := sha256.Sum256(planData)
    currentHash := hex.EncodeToString(hash[:])
    
    if currentHash != task.PlanHash {
        return a.handleWarmupError(task, 
            fmt.Errorf("plan hash mismatch: expected %s, got %s", 
                task.PlanHash, currentHash))
    }
    
    // æ ‡è®°ä¸ºready
    task.WarmupStatus = "ready"
    a.db.Save(task)
    
    log.Printf("[Agent] Task %d warmup completed successfully", task.ID)
    return nil
}

// 4. Applyæ‰§è¡Œé€»è¾‘ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
func (a *Agent) ExecuteApply(task *models.WorkspaceTask) error {
    log.Printf("[Agent] Executing apply for task %d", task.ID)
    
    workDir := fmt.Sprintf("/tmp/iac-platform/workspaces/%s/%d", 
        task.WorkspaceID, task.ID)
    
    // åœºæ™¯1: Agentæœªé”€æ¯ï¼Œå·¥ä½œç›®å½•å­˜åœ¨
    if task.WarmupAgentID != nil && *task.WarmupAgentID == a.agentID {
        log.Printf("[Agent] Same agent, checking local plan file...")
        
        planFile := filepath.Join(workDir, "plan.out")
        if _, err := os.Stat(planFile); err == nil {
            // éªŒè¯hash
            planData, err := os.ReadFile(planFile)
            if err == nil {
                hash := sha256.Sum256(planData)
                currentHash := hex.EncodeToString(hash[:])
                
                if currentHash == task.PlanHash {
                    log.Printf("[Agent]  Hash verified, using local plan file (FAST PATH)")
                    // ç›´æ¥æ‰§è¡Œapplyï¼Œæ— éœ€initï¼
                    return a.terraformApplyDirect(workDir)
                }
            }
            log.Printf("[Agent] Hash mismatch or read error, falling back to normal flow")
        }
    }
    
    // åœºæ™¯2: Agentå·²é”€æ¯æˆ–é¢„çƒ­å®Œæˆ
    if task.WarmupStatus == "ready" {
        log.Printf("[Agent]  Warmup ready, executing apply immediately")
        
        // éªŒè¯é¢„çƒ­æ˜¯å¦è¿‡æœŸ
        if task.WarmupExpiresAt != nil && time.Now().After(*task.WarmupExpiresAt) {
            log.Printf("[Agent] Warmup expired, re-warming...")
            if err := a.warmupTask(task); err != nil {
                return err
            }
        }
        
        return a.terraformApplyDirect(workDir)
    }
    
    // åœºæ™¯3: éœ€è¦å®Œæ•´å‡†å¤‡ï¼ˆfallbackï¼‰
    log.Printf("[Agent] No warmup, executing normal flow...")
    return a.executeApplyNormal(task)
}

// 5. ç›´æ¥æ‰§è¡Œapplyï¼ˆæœ€å¿«è·¯å¾„ï¼‰
func (a *Agent) terraformApplyDirect(workDir string) error {
    cmd := exec.Command("terraform", "apply", "-no-color", "-auto-approve", "plan.out")
    cmd.Dir = workDir
    
    var stdout, stderr bytes.Buffer
    cmd.Stdout = &stdout
    cmd.Stderr = &stderr
    
    log.Printf("[Agent] Executing: terraform apply (direct)")
    startTime := time.Now()
    
    if err := cmd.Run(); err != nil {
        return fmt.Errorf("terraform apply failed: %w\n%s", err, stderr.String())
    }
    
    duration := time.Since(startTime)
    log.Printf("[Agent]  Apply completed in %v (OPTIMIZED)", duration)
    
    return nil
}

// 6. å¤„ç†é¢„çƒ­é”™è¯¯
func (a *Agent) handleWarmupError(task *models.WorkspaceTask, err error) error {
    log.Printf("[Agent] âŒ Warmup failed for task %d: %v", task.ID, err)
    
    task.WarmupStatus = "failed"
    a.db.Save(task)
    
    // é¢„çƒ­å¤±è´¥ä¸å½±å“ä»»åŠ¡æ‰§è¡Œï¼Œç”¨æˆ·ç¡®è®¤åä¼šèµ°æ­£å¸¸æµç¨‹
    return err
}
```

### Agenté”€æ¯æ„ŸçŸ¥æœºåˆ¶

```go
// 1. Agentå¿ƒè·³æœºåˆ¶
func (a *Agent) StartHeartbeat(ctx context.Context) {
    ticker := time.NewTicker(30 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            a.sendHeartbeat()
        }
    }
}

// 2. æœåŠ¡ç«¯æ£€æµ‹Agentç¦»çº¿
func (m *TaskQueueManager) MonitorAgentHealth(ctx context.Context) {
    ticker := time.NewTicker(1 * time.Minute)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            m.checkOfflineAgents()
        }
    }
}

func (m *TaskQueueManager) checkOfflineAgents() {
    // æŸ¥æ‰¾è¶…è¿‡2åˆ†é’Ÿæœªå¿ƒè·³çš„Agent
    var offlineAgents []models.Agent
    m.db.Where("last_heartbeat_at < ?", time.Now().Add(-2*time.Minute)).
        Where("status = ?", "online").
        Find(&offlineAgents)
    
    for _, agent := range offlineAgents {
        log.Printf("[Monitor] Agent %s is offline, marking tasks for re-warmup", agent.AgentID)
        
        // å°†è¯¥Agenté¢„çƒ­çš„ä»»åŠ¡æ ‡è®°ä¸ºéœ€è¦é‡æ–°é¢„çƒ­
        m.db.Model(&models.WorkspaceTask{}).
            Where("warmup_agent_id = ?", agent.AgentID).
            Where("warmup_status = ?", "ready").
            Where("status = ?", models.TaskStatusApplyPending).
            Updates(map[string]interface{}{
                "warmup_status": "none",
                "warmup_agent_id": nil,
            })
        
        // æ ‡è®°Agentä¸ºoffline
        agent.Status = "offline"
        m.db.Save(&agent)
    }
}

// 3. æ–°Agentæ³¨å†Œæ—¶è§¦å‘é¢„çƒ­
func (h *AgentHandler) RegisterAgent(c *gin.Context) {
    // ... æ³¨å†Œé€»è¾‘
    
    // æ³¨å†ŒæˆåŠŸåï¼Œè§¦å‘é¢„çƒ­æ£€æŸ¥
    go func() {
        time.Sleep(5 * time.Second) // ç­‰å¾…Agentå®Œå…¨å¯åŠ¨
        agent.OnStart()
    }()
}
```

## ğŸ”„ ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ

### æ¨èå®æ–½æ–¹æ¡ˆ

ç»“åˆä¸¤ä¸ªä¼˜åŒ–ç‚¹ï¼Œæ¨èä»¥ä¸‹ç»¼åˆæ–¹æ¡ˆï¼ˆæ–¹æ¡ˆCï¼‰ï¼š

```go
// å®Œæ•´ä¼˜åŒ–æµç¨‹
Plané˜¶æ®µ:
1. åˆ›å»ºå·¥ä½œç›®å½•
2. ç”Ÿæˆé…ç½®æ–‡ä»¶
3. æ‹‰å–Stateæ–‡ä»¶
4. terraform init
5. terraform plan -out=plan.out
6. ä¿å­˜plan.outåˆ°æ•°æ®åº“
7. ä¿æŒå·¥ä½œç›®å½•ä¸æ¸…ç†  ä¼˜åŒ–1
8. ä»»åŠ¡çŠ¶æ€ â†’ apply_pending

Apply_Pendingé˜¶æ®µï¼ˆæ–°å¢ï¼‰:
1. å¦‚æœæ˜¯Agent/K8sæ¨¡å¼:
   - ç«‹å³æ¨é€é¢„çƒ­ä»»åŠ¡åˆ°Agent  ä¼˜åŒ–2
   - Agentåˆ›å»ºå·¥ä½œç›®å½•ï¼ˆæˆ–å¤ç”¨Plançš„ç›®å½•ï¼‰
   - Agentæ‰§è¡Œinitï¼ˆå¦‚æœéœ€è¦ï¼‰
   - Agentæ¢å¤plan.out
   - Agentæ ‡è®°ä¸ºready
2. ç­‰å¾…ç”¨æˆ·ç¡®è®¤

Applyé˜¶æ®µ:
1. ç”¨æˆ·ç¡®è®¤
2. Agentæ£€æŸ¥é¢„çƒ­çŠ¶æ€
3. å¦‚æœreadyï¼Œç«‹å³æ‰§è¡Œapply 
4. å¦‚æœæœªreadyï¼Œç­‰å¾…é¢„çƒ­å®Œæˆæˆ–é‡æ–°å‡†å¤‡
5. æ‰§è¡Œterraform apply
6. æ¸…ç†å·¥ä½œç›®å½•
```

### å®æ–½ä¼˜å…ˆçº§

**Phase 1: ä¼˜åŒ–ç‚¹1ï¼ˆä¿æŒå·¥ä½œç›®å½•ï¼‰**
- ä¼˜å…ˆçº§: P0ï¼ˆé«˜ï¼‰
- å·¥ä½œé‡: 2-3å¤©
- é£é™©: ä½
- æ”¶ç›Š: ä¸­ç­‰

**Phase 2: ä¼˜åŒ–ç‚¹2ï¼ˆAgenté¢„çƒ­ï¼‰**
- ä¼˜å…ˆçº§: P1ï¼ˆä¸­ï¼‰
- å·¥ä½œé‡: 5-7å¤©
- é£é™©: ä¸­ç­‰
- æ”¶ç›Š: é«˜

## ğŸš¨ éœ€è¦æ³¨æ„çš„é—®é¢˜

### 1. Agentè‡ªåŠ¨ç¼©å®¹ä¸é¢„çƒ­ä»»åŠ¡å†²çª

**é—®é¢˜**: Agentåœ¨æ²¡æœ‰runningä»»åŠ¡æ—¶ä¼šè‡ªåŠ¨ç¼©å®¹ï¼Œä½†apply_pendingä»»åŠ¡éœ€è¦Agentä¿æŒé¢„çƒ­çŠ¶æ€

**åœºæ™¯åˆ†æ**:
```
1. Planå®Œæˆ â†’ apply_pendingçŠ¶æ€
2. Agenté¢„çƒ­å®Œæˆ â†’ warmup_status = ready
3. æ²¡æœ‰runningä»»åŠ¡ â†’ Auto-scaleræ£€æµ‹åˆ°ç©ºé—²
4. Agentè¢«ç¼©å®¹é”€æ¯ âŒ
5. ç”¨æˆ·ç¡®è®¤Apply â†’ éœ€è¦é‡æ–°åˆ›å»ºAgentå¹¶é¢„çƒ­
```

**è§£å†³æ–¹æ¡ˆA: ä¿®æ”¹ç¼©å®¹é€»è¾‘ï¼ˆæ¨èï¼‰** 

```go
// åœ¨è®¡ç®—éœ€è¦çš„Agentæ•°é‡æ—¶ï¼Œè€ƒè™‘apply_pendingä»»åŠ¡
func (s *K8sDeploymentService) CalculateDesiredReplicas(poolID string) int {
    // 1. ç»Ÿè®¡runningä»»åŠ¡
    var runningCount int64
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusRunning).
        Count(&runningCount)
    
    // 2. ç»Ÿè®¡apply_pendingä»»åŠ¡ï¼ˆéœ€è¦ä¿æŒé¢„çƒ­ï¼‰
    var applyPendingCount int64
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusApplyPending).
        Where("workspace_tasks.warmup_status = ?", "ready"). // åªè®¡ç®—å·²é¢„çƒ­çš„
        Count(&applyPendingCount)
    
    // 3. è®¡ç®—æ‰€éœ€Agentæ•°é‡
    // æ¯ä¸ªAgentå¯ä»¥å¤„ç†3ä¸ªä»»åŠ¡ï¼ˆrunning + apply_pendingï¼‰
    totalTasks := runningCount + applyPendingCount
    desiredReplicas := (totalTasks + 2) / 3 // å‘ä¸Šå–æ•´
    
    // 4. æœ€å°å‰¯æœ¬æ•°
    if desiredReplicas < s.minReplicas {
        desiredReplicas = s.minReplicas
    }
    
    log.Printf("[AutoScaler] Pool %s: running=%d, apply_pending(ready)=%d, desired=%d",
        poolID, runningCount, applyPendingCount, desiredReplicas)
    
    return int(desiredReplicas)
}
```

**è§£å†³æ–¹æ¡ˆB: é¢„çƒ­è¿‡æœŸæ—¶é—´é…åˆç¼©å®¹å»¶è¿Ÿ**

```go
// 1. è®¾ç½®åˆç†çš„é¢„çƒ­è¿‡æœŸæ—¶é—´
task.WarmupExpiresAt = time.Now().Add(30 * time.Minute) // 30åˆ†é’Ÿ

// 2. ç¼©å®¹å»¶è¿Ÿæ—¶é—´åº”è¯¥å°äºé¢„çƒ­è¿‡æœŸæ—¶é—´
// ä¾‹å¦‚ï¼šç¼©å®¹å»¶è¿Ÿ15åˆ†é’Ÿï¼Œé¢„çƒ­è¿‡æœŸ30åˆ†é’Ÿ
// è¿™æ ·å³ä½¿Agentè¢«ç¼©å®¹ï¼Œé¢„çƒ­ä¹Ÿè¿˜æ²¡è¿‡æœŸï¼Œæ–°Agentå¯ä»¥é‡æ–°é¢„çƒ­

// 3. åœ¨Auto-scalerä¸­æ·»åŠ ç¼©å®¹å»¶è¿Ÿ
func (s *K8sDeploymentService) ShouldScaleDown(poolID string) bool {
    // æ£€æŸ¥æœ€åä¸€æ¬¡æœ‰ä»»åŠ¡çš„æ—¶é—´
    var lastTaskTime time.Time
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status IN (?)", []string{"running", "apply_pending"}).
        Select("MAX(workspace_tasks.updated_at)").
        Scan(&lastTaskTime)
    
    // 15åˆ†é’Ÿå†…æœ‰ä»»åŠ¡ï¼Œä¸ç¼©å®¹
    if time.Since(lastTaskTime) < 15*time.Minute {
        return false
    }
    
    return true
}
```

**è§£å†³æ–¹æ¡ˆC: æ··åˆæ–¹æ¡ˆ + é¢„çƒ­å¤±è´¥è®¡æ•°ï¼ˆæœ€ä½³ï¼‰** â­â­â­â­â­

```go
// ç»“åˆæ–¹æ¡ˆAå’ŒBï¼Œå¹¶æ·»åŠ é¢„çƒ­å¤±è´¥ä¿æŠ¤
func (s *K8sDeploymentService) CalculateDesiredReplicasV2(poolID string) int {
    // 1. ç»Ÿè®¡runningä»»åŠ¡
    var runningCount int64
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusRunning).
        Count(&runningCount)
    
    // 2. ç»Ÿè®¡apply_pendingä»»åŠ¡ï¼ˆå·²é¢„çƒ­ä¸”æœªè¿‡æœŸï¼Œä¸”é¢„çƒ­å¤±è´¥æ¬¡æ•°<3ï¼‰
    var applyPendingCount int64
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusApplyPending).
        Where("workspace_tasks.warmup_status = ?", "ready").
        Where("workspace_tasks.warmup_expires_at > ?", time.Now()).
        Where("COALESCE(workspace_tasks.warmup_retry_count, 0) < ?", 3). //  é˜²æ­¢æ­»å¾ªç¯
        Count(&applyPendingCount)
    
    // 3. è®¡ç®—æ‰€éœ€Agentæ•°é‡
    totalTasks := runningCount + applyPendingCount
    desiredReplicas := (totalTasks + 2) / 3
    
    // 4. åº”ç”¨æœ€å°å‰¯æœ¬æ•°
    if desiredReplicas < s.minReplicas {
        desiredReplicas = s.minReplicas
    }
    
    // 5. ç¼©å®¹ä¿æŠ¤ï¼šå¦‚æœè¦ç¼©å®¹ï¼Œæ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸå†…
    currentReplicas := s.getCurrentReplicas(poolID)
    if desiredReplicas < currentReplicas {
        if !s.canScaleDown(poolID) {
            log.Printf("[AutoScaler] Pool %s in cooldown, keeping current replicas", poolID)
            return currentReplicas
        }
    }
    
    return int(desiredReplicas)
}

// Agenté¢„çƒ­é€»è¾‘ï¼ˆæ·»åŠ é‡è¯•è®¡æ•°ï¼‰
func (a *Agent) warmupTask(task *models.WorkspaceTask) error {
    log.Printf("[Agent] Warming up task %d (retry count: %d)", task.ID, task.WarmupRetryCount)
    
    //  æ£€æŸ¥é‡è¯•æ¬¡æ•°ï¼Œé˜²æ­¢æ­»å¾ªç¯
    if task.WarmupRetryCount >= 3 {
        log.Printf("[Agent] Task %d warmup retry limit reached, giving up", task.ID)
        task.WarmupStatus = "failed"
        task.WarmupRetryCount = 3
        a.db.Save(task)
        return fmt.Errorf("warmup retry limit reached")
    }
    
    // å¢åŠ é‡è¯•è®¡æ•°
    task.WarmupRetryCount++
    task.WarmupStatus = "warming"
    task.WarmupAgentID = &a.agentID
    task.WarmupAt = timePtr(time.Now())
    task.WarmupExpiresAt = timePtr(time.Now().Add(30 * time.Minute))
    a.db.Save(task)
    
    // ... æ‰§è¡Œé¢„çƒ­é€»è¾‘
    
    // é¢„çƒ­æˆåŠŸï¼Œé‡ç½®é‡è¯•è®¡æ•°
    task.WarmupRetryCount = 0 //  æˆåŠŸåé‡ç½®
    task.WarmupStatus = "ready"
    a.db.Save(task)
    
    return nil
}

// Agentç¦»çº¿æ£€æµ‹ï¼ˆæ·»åŠ é‡è¯•è®¡æ•°æ£€æŸ¥ï¼‰
func (m *TaskQueueManager) checkOfflineAgents() {
    var offlineAgents []models.Agent
    m.db.Where("last_heartbeat_at < ?", time.Now().Add(-2*time.Minute)).
        Where("status = ?", "online").
        Find(&offlineAgents)
    
    for _, agent := range offlineAgents {
        log.Printf("[Monitor] Agent %s is offline", agent.AgentID)
        
        //  åªé‡ç½®é‡è¯•æ¬¡æ•°<3çš„ä»»åŠ¡ï¼Œé¿å…æ­»å¾ªç¯
        m.db.Model(&models.WorkspaceTask{}).
            Where("warmup_agent_id = ?", agent.AgentID).
            Where("warmup_status = ?", "ready").
            Where("status = ?", models.TaskStatusApplyPending).
            Where("COALESCE(warmup_retry_count, 0) < ?", 3). //  å…³é”®ä¿æŠ¤
            Updates(map[string]interface{}{
                "warmup_status": "none",
                "warmup_agent_id": nil,
            })
        
        agent.Status = "offline"
        m.db.Save(&agent)
    }
}
```

**å…³é”®æ”¹è¿›**:
1.  æ·»åŠ  `warmup_retry_count` å­—æ®µï¼Œè®°å½•é¢„çƒ­é‡è¯•æ¬¡æ•°
2.  é¢„çƒ­å¤±è´¥è¶…è¿‡3æ¬¡åï¼Œä¸å†å°è¯•é¢„çƒ­ï¼ˆé¿å…æ­»å¾ªç¯ï¼‰
3.  Auto-scaleråªè®¡ç®—é‡è¯•æ¬¡æ•°<3çš„ä»»åŠ¡
4.  é¢„çƒ­æˆåŠŸåé‡ç½®é‡è¯•è®¡æ•°
5.  Agentç¦»çº¿æ—¶åªé‡ç½®é‡è¯•æ¬¡æ•°<3çš„ä»»åŠ¡

**è§£å†³æ–¹æ¡ˆD: Podç›´æ¥ç®¡ç† + ä»»åŠ¡æ§½ä½æœºåˆ¶ï¼ˆæœ€ä¼˜æ¶æ„ï¼‰** â­â­â­â­â­

**æ ¸å¿ƒæ€æƒ³**: 
- ä¸ä½¿ç”¨Deploymentï¼Œç›´æ¥ç®¡ç†Pod
- æ¯ä¸ªPodæœ‰å¤šä¸ªä»»åŠ¡æ§½ä½ï¼ˆå¦‚3ä¸ªï¼‰
- apply_pendingä»»åŠ¡å ç”¨æ§½ä½ä½†ä¸ç®—running
- é¢„çƒ­é˜¶æ®µï¼ˆpre-applyï¼‰å ç”¨æ§½ä½ä¸”ç®—running
- Freeze Scheduleæ—¶å¼ºåˆ¶æ¸…ç†æ‰€æœ‰Pod

```go
// 1. Podä»»åŠ¡æ§½ä½ç®¡ç†
type PodSlot struct {
    SlotID    int       `json:"slot_id"`    // æ§½ä½ID (0, 1, 2)
    TaskID    *uint     `json:"task_id"`    // å½“å‰ä»»åŠ¡ID
    TaskType  string    `json:"task_type"`  // plan/plan_and_apply
    Status    string    `json:"status"`     // idle/running/reserved
    UpdatedAt time.Time `json:"updated_at"`
}

type ManagedPod struct {
    PodName       string     `json:"pod_name"`
    AgentID       string     `json:"agent_id"`
    PoolID        string     `json:"pool_id"`
    Slots         []PodSlot  `json:"slots"`         // 3ä¸ªæ§½ä½
    CreatedAt     time.Time  `json:"created_at"`
    LastHeartbeat time.Time  `json:"last_heartbeat"`
}

// 2. æ§½ä½è§„åˆ™
// - æ¯ä¸ªPodæœ‰3ä¸ªæ§½ä½
// - planä»»åŠ¡ï¼šå¯ä»¥å¹¶å‘ï¼Œå ç”¨1ä¸ªæ§½ä½
// - plan_and_applyä»»åŠ¡ï¼ˆrunningï¼‰ï¼šç‹¬å 1ä¸ªæ§½ä½
// - plan_and_applyä»»åŠ¡ï¼ˆapply_pendingï¼‰ï¼šå ç”¨1ä¸ªæ§½ä½ä½†æ ‡è®°ä¸ºreserved
// - é¢„çƒ­é˜¶æ®µï¼ˆpre-applyï¼‰ï¼šå ç”¨æ§½ä½ä¸”ç®—running

// 3. ä¼˜åŒ–åçš„Auto-scaleré€»è¾‘
func (s *K8sPodManager) ReconcilePods(poolID string) error {
    // 1. ç»Ÿè®¡éœ€è¦æ§½ä½çš„ä»»åŠ¡
    // runningçŠ¶æ€çš„ä»»åŠ¡ï¼ˆåŒ…æ‹¬planningå’Œapplyingé˜¶æ®µï¼‰
    var runningTasks []models.WorkspaceTask
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusRunning).
        Find(&runningTasks)
    
    // 2. ç»Ÿè®¡apply_pendingä»»åŠ¡ï¼ˆå·²é¢„çƒ­ï¼Œå ç”¨æ§½ä½ä½†ä¸ç®—runningï¼‰
    var applyPendingTasks []models.WorkspaceTask
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusApplyPending).
        Where("workspace_tasks.warmup_status = ?", "ready").
        Find(&applyPendingTasks)
    
    // 3. è®¡ç®—éœ€è¦çš„æ€»æ§½ä½æ•°
    totalSlots := len(runningTasks) + len(applyPendingTasks)
    
    // 4. è®¡ç®—éœ€è¦çš„Podæ•°é‡ï¼ˆæ¯ä¸ªPod 3ä¸ªæ§½ä½ï¼‰
    desiredPods := (totalSlots + 2) / 3 // å‘ä¸Šå–æ•´
    
    // 5. åº”ç”¨æœ€å°å‰¯æœ¬æ•°
    if desiredPods < s.minReplicas {
        desiredPods = s.minReplicas
    }
    
    // 6. è·å–å½“å‰Podåˆ—è¡¨
    currentPods := s.listPods(poolID)
    
    // 7. è°ƒæ•´Podæ•°é‡
    s.reconcilePods(poolID, desiredPods, currentPods, runningTasks, applyPendingTasks)
    
    log.Printf("[PodManager] Pool %s: running=%d, apply_pending=%d, total_slots=%d, pods=%d",
        poolID, len(runningTasks), len(applyPendingTasks), totalSlots, desiredPods)
    
    return nil
}

// 4. Podæ§½ä½åˆ†é…
func (s *K8sPodManager) reconcilePods(
    poolID string,
    desiredPods int,
    currentPods []ManagedPod,
    runningTasks []models.WorkspaceTask,
    applyPendingTasks []models.WorkspaceTask,
) error {
    // 1. æ‰©å®¹ï¼šåˆ›å»ºæ–°Pod
    if len(currentPods) < desiredPods {
        for i := len(currentPods); i < desiredPods; i++ {
            pod := s.createPod(poolID)
            log.Printf("[PodManager] Created pod %s", pod.PodName)
        }
    }
    
    // 2. ç¼©å®¹ï¼šåˆ é™¤ç©ºé—²Pod
    if len(currentPods) > desiredPods {
        // æ‰¾å‡ºå®Œå…¨ç©ºé—²çš„Podï¼ˆæ‰€æœ‰æ§½ä½éƒ½æ˜¯idleï¼‰
        idlePods := s.findIdlePods(currentPods)
        
        deleteCount := len(currentPods) - desiredPods
        for i := 0; i < deleteCount && i < len(idlePods); i++ {
            s.deletePod(idlePods[i].PodName)
            log.Printf("[PodManager] Deleted idle pod %s", idlePods[i].PodName)
        }
    }
    
    // 3. åˆ†é…runningä»»åŠ¡åˆ°æ§½ä½
    for _, task := range runningTasks {
        if task.AgentID == nil {
            // ä»»åŠ¡æœªåˆ†é…Agentï¼Œæ‰¾ä¸€ä¸ªæœ‰ç©ºé—²æ§½ä½çš„Pod
            pod := s.findPodWithFreeSlot(currentPods)
            if pod != nil {
                s.assignTaskToSlot(pod, &task, "running")
            }
        }
    }
    
    // 4. åˆ†é…apply_pendingä»»åŠ¡åˆ°æ§½ä½ï¼ˆæ ‡è®°ä¸ºreservedï¼‰
    for _, task := range applyPendingTasks {
        if task.WarmupAgentID == nil {
            // ä»»åŠ¡æœªé¢„çƒ­ï¼Œæ‰¾ä¸€ä¸ªæœ‰ç©ºé—²æ§½ä½çš„Pod
            pod := s.findPodWithFreeSlot(currentPods)
            if pod != nil {
                s.assignTaskToSlot(pod, &task, "reserved")
                // è§¦å‘é¢„çƒ­
                go s.triggerWarmup(pod, &task)
            }
        }
    }
    
    return nil
}

// 5. æŸ¥æ‰¾æœ‰ç©ºé—²æ§½ä½çš„Pod
func (s *K8sPodManager) findPodWithFreeSlot(pods []ManagedPod) *ManagedPod {
    for _, pod := range pods {
        for _, slot := range pod.Slots {
            if slot.Status == "idle" {
                return &pod
            }
        }
    }
    return nil
}

// 6. åˆ†é…ä»»åŠ¡åˆ°æ§½ä½
func (s *K8sPodManager) assignTaskToSlot(
    pod *ManagedPod, 
    task *models.WorkspaceTask, 
    slotStatus string,
) error {
    // æ‰¾åˆ°ç©ºé—²æ§½ä½
    for i, slot := range pod.Slots {
        if slot.Status == "idle" {
            pod.Slots[i].TaskID = &task.ID
            pod.Slots[i].TaskType = string(task.TaskType)
            pod.Slots[i].Status = slotStatus // "running" æˆ– "reserved"
            pod.Slots[i].UpdatedAt = time.Now()
            
            // æ›´æ–°ä»»åŠ¡çš„Agent ID
            if slotStatus == "running" {
                task.AgentID = &pod.AgentID
            } else if slotStatus == "reserved" {
                task.WarmupAgentID = &pod.AgentID
            }
            s.db.Save(task)
            
            log.Printf("[PodManager] Assigned task %d to pod %s slot %d (status: %s)",
                task.ID, pod.PodName, i, slotStatus)
            return nil
        }
    }
    return fmt.Errorf("no free slot available")
}

// 5. Freeze Scheduleå¤„ç†
func (s *FreezeScheduleService) EnterFreezeWindow(poolID string) error {
    log.Printf("[FreezeSchedule] Pool %s entering freeze window", poolID)
    
    // 1. æ ‡è®°Poolä¸ºfrozen
    s.db.Model(&models.AgentPool{}).
        Where("pool_id = ?", poolID).
        Update("is_frozen", true)
    
    // 2. å¼ºåˆ¶åˆ é™¤æ‰€æœ‰Podï¼ˆåŒ…æ‹¬Workerå’ŒReservedï¼‰
    pods := s.podManager.listPods(poolID)
    for _, pod := range pods {
        s.podManager.deletePod(pod.PodName)
        log.Printf("[FreezeSchedule] Deleted pod %s (freeze window)", pod.PodName)
    }
    
    // 3. å°†æ‰€æœ‰é¢„çƒ­ä»»åŠ¡æ ‡è®°ä¸ºéœ€è¦é‡æ–°é¢„çƒ­
    s.db.Model(&models.WorkspaceTask{}).
        Joins("JOIN workspaces ON workspaces.workspace_id = workspace_tasks.workspace_id").
        Where("workspaces.current_pool_id = ?", poolID).
        Where("workspace_tasks.status = ?", models.TaskStatusApplyPending).
        Updates(map[string]interface{}{
            "warmup_status": "none",
            "warmup_agent_id": nil,
        })
    
    log.Printf("[FreezeSchedule] Pool %s freeze window activated, all pods deleted", poolID)
    return nil
}

// 6. è§£å†»å¤„ç†
func (s *FreezeScheduleService) ExitFreezeWindow(poolID string) error {
    log.Printf("[FreezeSchedule] Pool %s exiting freeze window", poolID)
    
    // 1. æ ‡è®°Poolä¸ºunfrozen
    s.db.Model(&models.AgentPool{}).
        Where("pool_id = ?", poolID).
        Update("is_frozen", false)
    
    // 2. è§¦å‘Podé‡å»º
    s.podManager.ReconcilePods(poolID)
    
    log.Printf("[FreezeSchedule] Pool %s unfrozen, pods will be recreated", poolID)
    return nil
}
```

**ä¼˜ç‚¹**:
-  å®Œå…¨é¿å…æ­»å¾ªç¯ï¼ˆé¢„ç•™Podä¸ä¼šè¢«è‡ªåŠ¨ç¼©å®¹ï¼‰
-  é¢„ç•™Podä¸å ç”¨ä»»åŠ¡å®¹é‡
-  æ¶æ„æ›´æ¸…æ™°ï¼ˆWorker Pod vs Reserved Podï¼‰
-  Freeze Scheduleå¼ºåˆ¶æ¸…ç†æ‰€æœ‰Pod
-  è§£å†»åè‡ªåŠ¨é‡å»ºPodå’Œé¢„çƒ­

**ç¼ºç‚¹**:
-  éœ€è¦é‡æ„K8sç®¡ç†é€»è¾‘ï¼ˆä»Deploymentæ”¹ä¸ºç›´æ¥ç®¡ç†Podï¼‰
-  å®ç°å¤æ‚åº¦æœ€é«˜

**Podæ§½ä½æœºåˆ¶**:
```
æ¯ä¸ªPodæœ‰3ä¸ªæ§½ä½:

æ§½ä½çŠ¶æ€:
- idle: ç©ºé—²ï¼Œå¯ä»¥æ¥å—æ–°ä»»åŠ¡
- running: æ­£åœ¨æ‰§è¡Œä»»åŠ¡ï¼ˆplanæˆ–applyé˜¶æ®µï¼‰
- reserved: é¢„ç•™ç»™apply_pendingä»»åŠ¡ï¼ˆå·²é¢„çƒ­ï¼‰

å®¹é‡è®¡ç®—:
- runningçŠ¶æ€çš„plan+applyä»»åŠ¡ï¼šå ç”¨1ä¸ªæ§½ä½ï¼Œç®—running
- apply_pendingçŠ¶æ€çš„ä»»åŠ¡ï¼šå ç”¨1ä¸ªæ§½ä½ï¼Œä½†ä¸ç®—runningï¼ˆæ ‡è®°ä¸ºreservedï¼‰
- é¢„çƒ­é˜¶æ®µï¼ˆpre-applyï¼‰ï¼šå ç”¨æ§½ä½ä¸”ç®—running

ç¤ºä¾‹:
Pod-1: [running: task-1, reserved: task-2, idle]
- task-1: plan_and_apply, status=running, stage=applying
- task-2: plan_and_apply, status=apply_pending, warmup_status=ready
- æ§½ä½3: ç©ºé—²

Pod-2: [running: task-3, running: task-4, running: task-5]
- task-3/4/5: planä»»åŠ¡ï¼Œå¯ä»¥å¹¶å‘

ç¼©å®¹è§„åˆ™:
- åªåˆ é™¤æ‰€æœ‰æ§½ä½éƒ½æ˜¯idleçš„Pod
- æœ‰reservedæ§½ä½çš„Podä¸ä¼šè¢«åˆ é™¤
- æœ‰runningæ§½ä½çš„Podä¸ä¼šè¢«åˆ é™¤
```

**Freeze Scheduleè¡Œä¸º**:
```
è¿›å…¥Freeze Window:
1. æ ‡è®°Poolä¸ºfrozen
2. å¼ºåˆ¶åˆ é™¤æ‰€æœ‰Podï¼ˆWorker + Reservedï¼‰
3. é‡ç½®æ‰€æœ‰é¢„çƒ­çŠ¶æ€

é€€å‡ºFreeze Window:
1. æ ‡è®°Poolä¸ºunfrozen
2. è§¦å‘Podé‡å»º
3. è‡ªåŠ¨é¢„çƒ­apply_pendingä»»åŠ¡
```

**æ•°æ®åº“Schemaè¡¥å……**:
```sql
-- æ·»åŠ é¢„çƒ­é‡è¯•è®¡æ•°å­—æ®µ
ALTER TABLE workspace_tasks ADD COLUMN IF NOT EXISTS warmup_retry_count INTEGER DEFAULT 0;
```

**æ¨èé…ç½®**:
```yaml
auto_scaler:
  min_replicas: 1              # æœ€å°å‰¯æœ¬æ•°
  max_replicas: 10             # æœ€å¤§å‰¯æœ¬æ•°
  scale_up_delay: 30s          # æ‰©å®¹å»¶è¿Ÿ
  scale_down_delay: 15m        # ç¼©å®¹å»¶è¿Ÿï¼ˆé‡è¦ï¼ï¼‰
  warmup_expire_time: 30m      # é¢„çƒ­è¿‡æœŸæ—¶é—´
  warmup_max_retries: 3        # é¢„çƒ­æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰
  cooldown_period: 15m         # ç¼©å®¹å†·å´æœŸ
```

**å…³é”®ç‚¹**:
1.  ç¼©å®¹æ—¶è€ƒè™‘apply_pendingä»»åŠ¡
2.  åªè®¡ç®—å·²é¢„çƒ­ä¸”æœªè¿‡æœŸçš„ä»»åŠ¡
3.  **åªè®¡ç®—é¢„çƒ­é‡è¯•æ¬¡æ•°<3çš„ä»»åŠ¡ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰** â­
4.  è®¾ç½®åˆç†çš„ç¼©å®¹å»¶è¿Ÿï¼ˆ15åˆ†é’Ÿï¼‰
5.  é¢„çƒ­è¿‡æœŸæ—¶é—´åº”å¤§äºç¼©å®¹å»¶è¿Ÿ
6.  æ·»åŠ ç¼©å®¹å†·å´æœŸï¼Œé¿å…é¢‘ç¹ç¼©å®¹
7.  é¢„çƒ­å¤±è´¥3æ¬¡åæ”¾å¼ƒï¼Œç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†

### 2. å·¥ä½œç›®å½•ç®¡ç†

**é—®é¢˜**: ä¿æŒå·¥ä½œç›®å½•ä¼šå ç”¨ç£ç›˜ç©ºé—´

**è§£å†³æ–¹æ¡ˆ**:
```go
// å®šæœŸæ¸…ç†è¿‡æœŸçš„å·¥ä½œç›®å½•
func (m *TaskQueueManager) CleanupExpiredWorkDirs() {
    // æ¸…ç†è¶…è¿‡24å°æ—¶çš„apply_pendingä»»åŠ¡çš„å·¥ä½œç›®å½•
    // æ¸…ç†è¶…è¿‡1å°æ—¶çš„å·²å®Œæˆä»»åŠ¡çš„å·¥ä½œç›®å½•
}
```

### 2. Agenté‡å¯/é”€æ¯

**é—®é¢˜**: Agenté‡å¯åï¼Œé¢„çƒ­çš„å·¥ä½œç›®å½•ä¸¢å¤±

**è§£å†³æ–¹æ¡ˆ**:
```go
// Agentå¯åŠ¨æ—¶æ£€æŸ¥é¢„çƒ­ä»»åŠ¡
func (a *Agent) OnStart() {
    // 1. æŸ¥è¯¢åˆ†é…ç»™è‡ªå·±çš„apply_pendingä»»åŠ¡
    // 2. é‡æ–°æ‰§è¡Œé¢„çƒ­
    // 3. æ ‡è®°ä¸ºready
}
```

### 3. é¢„çƒ­è¿‡æœŸ

**é—®é¢˜**: é¢„çƒ­åé•¿æ—¶é—´æœªç¡®è®¤ï¼Œé…ç½®å¯èƒ½å·²å˜æ›´

**è§£å†³æ–¹æ¡ˆ**:
```go
// è®¾ç½®é¢„çƒ­è¿‡æœŸæ—¶é—´ï¼ˆå¦‚1å°æ—¶ï¼‰
task.WarmupExpiresAt = time.Now().Add(1 * time.Hour)

// ç”¨æˆ·ç¡®è®¤æ—¶æ£€æŸ¥
if task.WarmupStatus == WarmupStatusReady {
    if time.Now().After(*task.WarmupExpiresAt) {
        // é¢„çƒ­å·²è¿‡æœŸï¼Œé‡æ–°å‡†å¤‡
        return a.executeApplyNormal(taskID)
    }
}
```

### 4. Planæ–‡ä»¶å®Œæ•´æ€§

**é—®é¢˜**: ä¿æŒå·¥ä½œç›®å½•æœŸé—´ï¼Œplan.outå¯èƒ½è¢«ç¯¡æ”¹

**è§£å†³æ–¹æ¡ˆ**:
```go
// ä¿å­˜plan.outæ—¶è®¡ç®—hash
planData, _ := os.ReadFile("plan.out")
task.PlanHash = sha256.Sum256(planData)

// Applyå‰éªŒè¯hash
currentHash := sha256.Sum256(planData)
if currentHash != task.PlanHash {
    return errors.New("plan file corrupted")
}
```

## ğŸ“Š æ€§èƒ½æå‡é¢„ä¼°

### ä¼˜åŒ–ç‚¹1: ä¿æŒå·¥ä½œç›®å½•

| åœºæ™¯ | å½“å‰è€—æ—¶ | ä¼˜åŒ–åè€—æ—¶ | æå‡ |
|------|---------|-----------|------|
| å°å‹é…ç½® | 15-20ç§’ | 2-3ç§’ | 85% |
| ä¸­å‹é…ç½® | 30-45ç§’ | 2-3ç§’ | 93% |
| å¤§å‹é…ç½® | 60-90ç§’ | 2-3ç§’ | 96% |

**è¯´æ˜**: ä¸»è¦èŠ‚çœinitæ—¶é—´ï¼ˆä¸‹è½½Provideræ’ä»¶ï¼‰

### ä¼˜åŒ–ç‚¹2: Agenté¢„çƒ­

| åœºæ™¯ | å½“å‰ä½“éªŒ | ä¼˜åŒ–åä½“éªŒ | æå‡ |
|------|---------|-----------|------|
| ç”¨æˆ·ç¡®è®¤åç­‰å¾… | 15-60ç§’ | <1ç§’ | 98% |
| Agenté‡å¯åœºæ™¯ | 30-90ç§’ | <1ç§’ | 99% |

**è¯´æ˜**: ç”¨æˆ·æ„ŸçŸ¥çš„ç­‰å¾…æ—¶é—´å‡ ä¹ä¸º0

## ğŸ“ æ€»ç»“

### åˆç†æ€§è¯„ä¼°

ä¸¤ä¸ªä¼˜åŒ–ç‚¹éƒ½**éå¸¸åˆç†**ï¼š

1. **ä¼˜åŒ–ç‚¹1ï¼ˆä¿æŒå·¥ä½œç›®å½•ï¼‰**:
   -  æŠ€æœ¯ä¸Šå®Œå…¨å¯è¡Œ
   -  æ€§èƒ½æå‡æ˜æ˜¾
   -  å®ç°å¤æ‚åº¦ä½
   -  é£é™©å¯æ§
   - **æ¨èç«‹å³å®æ–½**

2. **ä¼˜åŒ–ç‚¹2ï¼ˆAgenté¢„çƒ­ï¼‰**:
   -  æŠ€æœ¯ä¸Šå¯è¡Œ
   -  ç”¨æˆ·ä½“éªŒæå‡å·¨å¤§
   -  å®ç°å¤æ‚åº¦ä¸­ç­‰
   -  éœ€è¦ä»”ç»†å¤„ç†è¾¹ç•Œæƒ…å†µ
   - **æ¨èä½œä¸ºPhase 2å®æ–½**

### å®æ–½å»ºè®®

1. **å…ˆå®æ–½ä¼˜åŒ–ç‚¹1**: å¿«é€Ÿè§æ•ˆï¼Œé£é™©ä½
2. **å†å®æ–½ä¼˜åŒ–ç‚¹2**: éœ€è¦æ›´å¤šæµ‹è¯•å’ŒéªŒè¯
3. **é€æ­¥æ¨å¹¿**: å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯ï¼Œå†æ¨å¹¿åˆ°ç”Ÿäº§

### é¢„æœŸæ”¶ç›Š

- **æ€§èƒ½**: Applyå¯åŠ¨æ—¶é—´å‡å°‘85-96%
- **ç”¨æˆ·ä½“éªŒ**: ç¡®è®¤åå‡ ä¹ç«‹å³å¼€å§‹æ‰§è¡Œ
- **èµ„æº**: å‡å°‘é‡å¤çš„ç½‘ç»œå’Œç£ç›˜IO
- **æˆæœ¬**: å‡å°‘Agentæ‰§è¡Œæ—¶é—´ï¼Œé™ä½æˆæœ¬

---

**ç›¸å…³æ–‡æ¡£**:
- [terraform-execution-states-and-sequential-guarantee.md](terraform-execution-states-and-sequential-guarantee.md) - æ‰§è¡Œæµç¨‹çŠ¶æ€
- [15-terraform-execution-detail.md](workspace/15-terraform-execution-detail.md) - æ‰§è¡Œæµç¨‹è®¾è®¡
